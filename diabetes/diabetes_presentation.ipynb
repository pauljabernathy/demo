{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This analysis uses the <a href=\"https://www.kaggle.com/uciml/pima-indians-diabetes-database?select=diabetes.csv\">Pima Indians Diabetes Dataset</a> downloaded from Kaggle.  It contains measurements of 768 women of Pima ancestry.  The target variable is whether or not they had diabetes.  The other 8 attributes are input variables.\n",
    "    \n",
    "In this notebook, we will attempt to train 3 different machine learning models to predict whether or not the patient has diabetes based on the other attributes given.  The algorithms used will be <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Logistic Regression</a>, <a href=\"https://en.wikipedia.org/wiki/Random_forest\">Random Forests</a>, and <a href=\"https://en.wikipedia.org/wiki/Neural_network\">Neural Networks</a>.  We will use scikit-learn for Logistic Regression and Random Forests.  For Neural Networks will will use Keras with Tensorflow underneath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the usual suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes and plots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn data splitting and prediction analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# the models used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the versions being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn 0.23.2\n",
      "keras 2.3.1\n",
      "tensorflow 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('sklearn', sklearn.__version__)\n",
    "import keras\n",
    "print('keras', keras.__version__)\n",
    "import tensorflow\n",
    "print('tensorflow', tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>The data set</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the dataframe.  You could do <code>\"diab = pd.read_csv('diabetes.csv')\"</code>.  But I wrote a function that loads the data and changes the column names to all lower case letters (a personal preference - makes it easier to type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(file_name):\n",
    "    '''\n",
    "    Load the dataset from the csv file.\n",
    "    :return: a DataFrame containing the dataset\n",
    "    '''\n",
    "    diab = pd.read_csv(file_name)\n",
    "    new_column_names = {}\n",
    "    for column in diab.columns:\n",
    "        new_column_names[column] = column.lower()\n",
    "    diab = diab.rename(columns=new_column_names)\n",
    "    return diab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab = get_df('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bloodpressure</th>\n",
       "      <th>skinthickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>diabetespedigreefunction</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  bloodpressure  skinthickness  insulin   bmi  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   diabetespedigreefunction  age  outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.651042\n",
       "1    0.348958\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab.outcome.value_counts() / diab.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this data set, 500 (about 65%) were not diagnosed as diabetic and 268 (about 35%) were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am interested to know how correlated each input variable is to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(diab.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little difficult to read, especially if you are only intrested in correlations between the inputs and the target, not between inputs and other inputs, so let's list the correlations of each input to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glucose', 0.4665813983068733),\n",
       " ('bmi', 0.29269466264444505),\n",
       " ('age', 0.23835598302719765),\n",
       " ('pregnancies', 0.2218981530339865),\n",
       " ('diabetespedigreefunction', 0.17384406565295976),\n",
       " ('insulin', 0.13054795488404788),\n",
       " ('skinthickness', 0.07475223191831944),\n",
       " ('bloodpressure', 0.06506835955033273)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = []\n",
    "for column in diab.columns[:-1]:\n",
    "    correlations.append((column,  diab.outcome.corr(diab[column])))\n",
    "correlations.sort(key = lambda kv: kv[1], reverse=True)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Data Preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using some algorithms liks logistic regression and neural networks, it often works best to scan the columns, often to between -1 and 1 or 0 and 1.  Another possbility is to scale the values by how many standard deviations they are from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diab.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not use the standard deviation method here because most of the variables are not normally distributed.  (The easiest way to see that is to view a graph of all the variables, which is not included here to save space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_scale_column(column):\n",
    "    '''\n",
    "    Scale the column so all the values are between 0 and 1.\n",
    "    :param column:\n",
    "    :return:\n",
    "    '''\n",
    "    column = pd.Series(column)  # make sure it is a series so we can do a vectorized operation\n",
    "    min = column.min()\n",
    "    max = column.max()\n",
    "    column = column.apply(lambda n: (n - min) / (max - min))\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in ['pregnancies',\n",
    " 'glucose',\n",
    " 'bloodpressure',\n",
    " 'skinthickness',\n",
    " 'insulin',\n",
    " 'bmi',\n",
    " 'diabetespedigreefunction',\n",
    " 'age']:\n",
    "    diab[column_name] = linear_scale_column(diab[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Uncomment to show all the graphs.\n",
    "for column in list(diab.columns[:-1]):\n",
    "    if diab[column].dtype == 'int64':\n",
    "        sns.countplot(diab[column])\n",
    "    else:\n",
    "        sns.distplot(diab[column])\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diab.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now they are all between 0 and 1.  The correlations are stil the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glucose', 0.4665813983068734),\n",
       " ('bmi', 0.2926946626444452),\n",
       " ('age', 0.23835598302719743),\n",
       " ('pregnancies', 0.22189815303398672),\n",
       " ('diabetespedigreefunction', 0.17384406565295984),\n",
       " ('insulin', 0.1305479548840473),\n",
       " ('skinthickness', 0.0747522319183193),\n",
       " ('bloodpressure', 0.06506835955033263)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = []\n",
    "for column in diab.columns[:-1]:\n",
    "    correlations.append((column,  diab.outcome.corr(diab[column])))\n",
    "correlations.sort(key = lambda kv: kv[1], reverse=True)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data set, there was no other data manipulation necessary, so we can go on to training the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Training the Algorithms</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll start training algorithms and doing predictions.\n",
    "\n",
    "The first step is to split the data into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(diab[diab.columns[:-1]], diab.outcome, test_size=.3, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save some typing, let's make a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(diab):\n",
    "    '''\n",
    "    Split the data into the train and test portions.  This function really isn't all that necessary in it's current\n",
    "    form.  It just saves a little bit of typing.\n",
    "    :param diab:\n",
    "    :return: a four part tuple\n",
    "    '''\n",
    "    x_train, x_test, y_train, y_test = train_test_split(diab[diab.columns[:-1]], diab.outcome, test_size=.3, random_state=37)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression and Random Forests</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train and evaluate a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n",
      "[[141  10]\n",
      " [ 41  39]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr_predictions = lr.predict(x_test)\n",
    "print(accuracy_score(y_test, lr_predictions))\n",
    "print(confusion_matrix(y_test, lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it gets almost 80% correct.  Let's compare it's prediction proportions to the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.65368\n",
       "1    0.34632\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts() / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.787879\n",
       "1    0.212121\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lr_predictions).value_counts() / len(lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Logistic Regression model is predicting more negatives than it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try Randoms Forests.  Since Random Forests have some randomness, the results can changes slightly from one run to the next.  So let's make a function to run the training and predicting so we can do it with just one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7662337662337663\n",
      "[[135  16]\n",
      " [ 38  42]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "rf_predictions = rf.predict(x_test)\n",
    "print(accuracy_score(y_test, rf_predictions))\n",
    "print(confusion_matrix(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But due to the radomness involved, there is some variation in Random Forest training.  So it would be nice to be able to run a machine learning algorithm multiple times without having to type the same lines over and over again.  So let's write a few functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_one_run(model, x_train, x_test, y_train, y_test, print_only=False):\n",
    "    '''\n",
    "    Do one train / test cycle of the given model with the given data.\n",
    "    :param model: the sklearn model\n",
    "    :param x_train: the DataFrame holding the input variables of the training set\n",
    "    :param x_test: the DataFrame holding the input variables of the test set\n",
    "    :param y_train: the target column of the training set\n",
    "    :param y_test: the target column of the test set\n",
    "    :param print_only: whether you should print the results or return for later aggregation\n",
    "    :return:\n",
    "    '''\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    if print_only:\n",
    "        show_accuracy_stats(y_test, predictions)\n",
    "    else:\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_forest_single(diab):\n",
    "    '''\n",
    "    Run one train / test cycle for Random Forests and print the results.\n",
    "    :param diab: the DataFrame\n",
    "    :return: nothing\n",
    "    '''\n",
    "    rf = RandomForestClassifier()\n",
    "    x_train, x_test, y_train, y_test = get_train_test_split(diab)\n",
    "    do_one_run(rf, x_train, x_test, y_train, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_logistic_single(diab):\n",
    "    '''\n",
    "    Run one train / test cycle for Logistic Regression and print the results.\n",
    "    :param diab: the DataFrame\n",
    "    :return:  nothing\n",
    "    '''\n",
    "    lr = LogisticRegression(max_iter=5000)\n",
    "    x_train, x_test, y_train, y_test = get_train_test_split(diab)\n",
    "    do_one_run(lr, x_train, x_test, y_train, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy_stats(y_test, predictions):\n",
    "    print(accuracy_score(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n",
      "[[141  10]\n",
      " [ 41  39]]\n"
     ]
    }
   ],
   "source": [
    "do_logistic_single(diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575757575757576\n",
      "[[131  20]\n",
      " [ 36  44]]\n"
     ]
    }
   ],
   "source": [
    "do_random_forest_single(diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7489177489177489\n",
      "[[133  18]\n",
      " [ 40  40]]\n"
     ]
    }
   ],
   "source": [
    "do_random_forest_single(diab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the slightly different results for Random Forests.\n",
    "\n",
    "Now, let's write functions to run it multiple times and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictions(predictions_list, one_prob):\n",
    "    '''\n",
    "    :param predictions_list:\n",
    "    :param one_prob: the probability of getting one, if you randomly choose 1 or 0\n",
    "    :return:\n",
    "    '''\n",
    "    predictions_np = np.array(predictions_list)\n",
    "    predictions_mean = (np.apply_along_axis(sum, 0, predictions_np) / predictions_np.shape[0]).reshape(1, predictions_np.shape[1])\n",
    "    agg = np.apply_along_axis(lambda n: 1 if n > one_prob else (np.random.choice([1, 0], p=[one_prob, 1 - one_prob]) if n == one_prob else 0), 0, predictions_mean)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aggregations(model, x_train, x_test, y_train, y_test, the_function=do_one_run, num_runs=10):\n",
    "    '''\n",
    "    Run the model many times, using the given function, and aggregate the results.  Really, \"average\" might be the\n",
    "    better word.\n",
    "    Currently, the function parameter is not necessary.  I had intended this to be used for any of the model types\n",
    "    (logistic, random forests, and neural networks), but neural networks required a slight different function,\n",
    "    so I added the function parameter.  Then I discovered that the neural network algorithm needs a new model each\n",
    "    time, so I just made the do_dnn_many() function to be the neural network equivalent of this function.\n",
    "    :param model: The sklearn model.  In this case, either LogisticRegression or RandomForest.\n",
    "    :param the_function: The function to use.\n",
    "    :param num_runs: How many times to run it.\n",
    "    :return:\n",
    "    '''\n",
    "    lists = []\n",
    "    for i in range(num_runs):\n",
    "        result = the_function(model, x_train, x_test, y_train, y_test)\n",
    "        lists.append(result)\n",
    "        #show_accuracy_stats(y_test, result)\n",
    "\n",
    "    predictions_np = np.array(lists)\n",
    "    one_prob = sum(y_test) / len(y_test)\n",
    "    agg = aggregate_predictions(predictions_np, one_prob)\n",
    "    print(\"\\naggregate results:\")\n",
    "    show_accuracy_stats(y_test, agg)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And functions to run Logistic Regression and Random Forests many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_logistic_many(diab):\n",
    "    '''\n",
    "    Do many train / test cycles of logistic regression.  Not really needed since it is the same for each run.\n",
    "    :param diab:\n",
    "    :return:\n",
    "    '''\n",
    "    print(\"\\nrunning logistic regressions:\")\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "    x_train, x_test, y_train, y_test = get_train_test_split(diab)\n",
    "    return run_aggregations(model, x_train, x_test, y_train, y_test, the_function=do_one_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_forest_many(diab):\n",
    "    '''\n",
    "    Do many train / test cycles of Random Forests and print the aggregated results.\n",
    "    :param diab:\n",
    "    :return:\n",
    "    '''\n",
    "    print(\"\\nrunning random forests:\")\n",
    "    model = RandomForestClassifier()\n",
    "    x_train, x_test, y_train, y_test = get_train_test_split(diab)\n",
    "    return run_aggregations(model, x_train, x_test, y_train, y_test, the_function=do_one_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running random forests:\n",
      "\n",
      "aggregate results:\n",
      "0.7619047619047619\n",
      "[[131  20]\n",
      " [ 35  45]]\n"
     ]
    }
   ],
   "source": [
    "rf = do_random_forest_many(diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running logistic regressions:\n",
      "\n",
      "aggregate results:\n",
      "0.7792207792207793\n",
      "[[141  10]\n",
      " [ 41  39]]\n"
     ]
    }
   ],
   "source": [
    "lg = do_logistic_many(diab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the Logistic Regression results do not change from one run to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Networks</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's turn our attention to Neural Networks.\n",
    "\n",
    "I'll create a simple Neural Network model with 8 inputs (one for each independent variable).  We can change the number of layers based on a parameter in the function below, and the number of nodes in the hidden layers can also be specified.  Foe example, a call to make_dnn_model(2, 4) will lead to a network with 8 input nodes, a layer with 32 nodes, another layer with 32 nodes, and an output layer.\n",
    "\n",
    "As with Random Forests, there is some randomness involved in training the neural network so the results are not deterministic.  So I'll make functions to run it multiple times.  Because the Neural Network is run in a slightly different way that the Random Forest, I can't reuse the run_aggregations() function from before, so do_dnn_many will combine the work of run_X_many() and run_aggregations()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dnn_model(num_layers, layer_size_ratio):\n",
    "    '''\n",
    "    make a very simple, or even simplistic, neural network model\n",
    "    :param num_layers: how many layers\n",
    "    :param layer_size_ratio: ratio of size of middle layers to the input layer\n",
    "    :return:\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    layer_width = 8 * layer_size_ratio\n",
    "    # TODO:  Don't hard code 8 (the number of input variables in this particular case) in here\n",
    "    model.add(Dense(layer_width, input_dim=8, activation='relu'))\n",
    "    for i in range(num_layers - 1):\n",
    "        model.add(Dense(layer_width, input_dim=layer_width, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_one_dnn_run(model, x_train, x_test, y_train, y_test, num_epochs=100, print_only=False):\n",
    "    '''\n",
    "    Do one train / test run of the given neural network model.\n",
    "    :param num_epochs: how many epochs to use in the neural network training\n",
    "    :param print_only: to print the results or return them for aggregation later\n",
    "    :return:\n",
    "    '''\n",
    "    y_binary = to_categorical(y_train)\n",
    "    model.fit(x_train, y_binary, epochs=num_epochs, verbose=0)\n",
    "\n",
    "    dnn_predictions = model.predict(x_test)\n",
    "\n",
    "    pred = np.apply_along_axis(lambda r: 1 if r[1] > r[0] else 0, 1, dnn_predictions)\n",
    "    if print_only:\n",
    "        show_accuracy_stats(y_test, pred)\n",
    "    else:\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dnn_single(diab):\n",
    "    '''\n",
    "    Make a DNN model and run it once.\n",
    "    :param diab:\n",
    "    :return:\n",
    "    '''\n",
    "    model = make_dnn_model(5, 4)\n",
    "    x_train, x_test, y_train, y_test = get_train_test_split(diab)\n",
    "    do_one_dnn_run(model, x_train, x_test, y_train, y_test, print_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dnn_many(diab, num_runs=10, num_layers=5, layer_size_ratio=4):\n",
    "    '''\n",
    "    Run the DNN train / test cycle many times and aggregate the results.\n",
    "    :param diab:\n",
    "    :return:\n",
    "    '''\n",
    "    print(\"\\ndoing DNN:\")\n",
    "    x_train, x_test, y_train, y_test = get_train_test_split(diab)\n",
    "    lists = []\n",
    "    for i in range(num_runs):\n",
    "        model = make_dnn_model(num_layers, layer_size_ratio)\n",
    "        result = do_one_dnn_run(model, x_train, x_test, y_train, y_test)\n",
    "        lists.append(result)\n",
    "        #show_accuracy_stats(y_test, result)\n",
    "\n",
    "    predictions_np = np.array(lists)\n",
    "    one_prob = sum(y_test) / len(y_test)\n",
    "    agg = aggregate_predictions(predictions_np, one_prob)\n",
    "    print(\"\\naggregate results:\")\n",
    "    show_accuracy_stats(y_test, agg)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing DNN:\n",
      "\n",
      "aggregate results:\n",
      "0.7489177489177489\n",
      "[[114  37]\n",
      " [ 21  59]]\n"
     ]
    }
   ],
   "source": [
    "dnn_predictions_5_4 = do_dnn_many(diab, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing DNN:\n",
      "\n",
      "aggregate results:\n",
      "0.7878787878787878\n",
      "[[133  18]\n",
      " [ 31  49]]\n"
     ]
    }
   ],
   "source": [
    "dnn_predictions_10_2 = do_dnn_many(diab, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doing DNN:\n",
      "\n",
      "aggregate results:\n",
      "0.7359307359307359\n",
      "[[117  34]\n",
      " [ 27  53]]\n"
     ]
    }
   ],
   "source": [
    "dnn_predictions_10_4 = do_dnn_many(diab, 10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the few examples I have tried, (10, 2) seems the best parameter set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<h2>Comparing the three algorithms</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at each of them together, and the aggregated results from all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running logistic regressions:\n",
      "\n",
      "aggregate results:\n",
      "0.7792207792207793\n",
      "[[141  10]\n",
      " [ 41  39]]\n",
      "\n",
      "running random forests:\n",
      "\n",
      "aggregate results:\n",
      "0.7489177489177489\n",
      "[[131  20]\n",
      " [ 38  42]]\n",
      "\n",
      "doing DNN:\n",
      "\n",
      "aggregate results:\n",
      "0.7748917748917749\n",
      "[[129  22]\n",
      " [ 30  50]]\n"
     ]
    }
   ],
   "source": [
    "lg_predictions = do_logistic_many(diab)\n",
    "rf_predictions = do_random_forest_many(diab)\n",
    "dnn_predictions = do_dnn_many(diab, 10, 2)\n",
    "agg = aggregate_predictions([lg_predictions, rf_predictions, dnn_predictions], 0.348958)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7835497835497836\n",
      "[[135  16]\n",
      " [ 34  46]]\n"
     ]
    }
   ],
   "source": [
    "show_accuracy_stats(y_test, agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "    \n",
    "In this dataset, using a Logistic Regression model with scikit-learn, we can predict whether or not a woman was diagnosed with diabetes, based on the other variables given, with about 77.9% accuracy.  Logistic Regression was more accurate than Random Forests and most of the attemps with Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my observations of the Machine Learning community, it seems that Neural Networks and Random Forests are very popular algorithms.  And they do perform well in many circumstances.  However, in this case, they did not outperform Logistic Regression and take more time and electricity to train (especially Neural Networks).    I don't have much experience with Neural Networks thought, so I remain very open to the possibility that there is a better way to set up and run the network.\n",
    "\n",
    "However despite Logistic Regression having a higher accuracy score, more True Negatives, and fewer False Positives, it does have more False Negatives and fewer True Positives than the others.  It seems that among the cases that actually are Negatives (that is, women without diabetes, which almost 2/3 are), Logistic Regression does the best job of determining that it is negative.  But for those that are positive, Random Forests and Neural Netorks do a better job.\n",
    "\n",
    "In a medical diagnosis, a False Negative could be more harmful than a False Positive.  It could depend on the illness and what actions are taken after a positive diagnosis (retest, invasive treatment, etc.).  But False Positives can be dangerous if they lead someone who is sick to not get treatment.\n",
    "\n",
    "So if False Negatives are a greater problem than False Positives, then Logisitc Regression may not be the best algorithm to use in this case, even though it is slightly more accurate.\n",
    "\n",
    "One possible candidate would be the aggregation of all three models, which has an overall accuracy nearly identical to Logistic Regression but with fewer False Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>For further research:</h3>\n",
    "\n",
    "<ul>\n",
    "    <li>More experimenting with Neural Networks.  I am still someone new to it.</li>\n",
    "    <li>Experiment with parameter tuning on Random Forests.</li>\n",
    "    <li>A similar analysis with a larger data set.</li>\n",
    "    <li>Train with different train/test splits.</li>\n",
    "    <li>Principal Component Analysis - can we get it down from 8 inputs to, say, 5?</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
